{"cells":[{"cell_type":"markdown","id":"4a7d4c79","metadata":{},"source":["### Task-I: Build and populate necessary tables (30% of course project grade)\n","1) Ingest both train and test data into one Postgres Database table. Use the augmented datasets that are provided under Final CSV folder.\n","2) Add a field to your database table that distinguishes between train and test datasets.\n","3) Identify constraints as needed and document them in your Readme.md file.\n","4) Your tables should be created in schema with the name “mqtt”.\n","5) In your ReadMe.md, add a description for the features in the dataset.\n","6) Use the reduced version of the data if your laptop’s memory can’t handle the original dataset."]},{"cell_type":"code","execution_count":1,"id":"74d6158a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/11/16 19:05:09 INFO SparkEnv: Registering MapOutputTracker\n","23/11/16 19:05:09 INFO SparkEnv: Registering BlockManagerMaster\n","23/11/16 19:05:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","23/11/16 19:05:09 INFO SparkEnv: Registering OutputCommitCoordinator\n","/usr/lib/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]}],"source":["# Uncomment the following lines if you are using Windows!\n","# import findspark\n","# findspark.init()\n","# findspark.find()\n","\n","import pyspark\n","\n","from pyspark.sql import SparkSession\n","from pyspark import SparkContext, SQLContext\n","\n","appName = \"Big Data Analytics\"\n","master = \"yarn\"\n","\n","# Create Configuration object for Spark.\n","conf = pyspark.SparkConf()\\\n","    .set('spark.driver.host','127.0.0.1')\\\n","    .setAppName(appName)\\\n","    .setMaster(master)\\\n","    .set('spark.jars', 'gs://dataproc-staging-us-central1-159964990471-2n8oqiw8/postgresql-42.6.0.jar')\n","\n","# Create Spark Context with the new configurations rather than relying on the default one\n","sc = SparkContext.getOrCreate(conf=conf)\n","\n","# You need to create SQL Context to conduct some database operations like what we will see later.\n","sqlContext = SQLContext(sc)\n","\n","# If you have SQL context, you create the session from the Spark Context\n","spark = sqlContext.sparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","id":"eec10af5","metadata":{},"source":["This Schema has been created after manually observing the different values in the .csv file. For Hexadecimal vals, string type is used and for mqtt_msg string type is used."]},{"cell_type":"code","execution_count":2,"id":"8866ab5b","metadata":{},"outputs":[],"source":["sqlWay = spark.sql(\"\"\"\n","CREATE SCHEMA mqtt;\n","\"\"\")\n","\n","# sqlWay.show()"]},{"cell_type":"code","execution_count":3,"id":"e4c0a790","metadata":{},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n","\n","\n","mqtt = StructType([\n","    StructField(\"tcp_flags\", StringType(), True),\n","    StructField(\"tcp_time_delta\", DoubleType(), True),\n","    StructField(\"tcp_len\", IntegerType(), True),\n","    StructField(\"mqtt_conack_flags\", StringType(), True),\n","    StructField(\"mqtt_conack_flags_reserved\", IntegerType(), True),\n","    StructField(\"mqtt_conack_flags_sp\", IntegerType(), True),\n","    StructField(\"mqtt_conack_val\", IntegerType(), True),\n","    StructField(\"mqtt_conflag_cleansess\", IntegerType(), True),\n","    StructField(\"mqtt_conflag_passwd\", IntegerType(), True),\n","    StructField(\"mqtt_conflag_qos\", IntegerType(), True),\n","    StructField(\"mqtt_conflag_reserved\", IntegerType(), True),\n","    StructField(\"mqtt_conflag_retain\", IntegerType(), True),\n","    StructField(\"mqtt_conflag_uname\", IntegerType(), True),\n","    StructField(\"mqtt_conflag_willflag\", IntegerType(), True),\n","    StructField(\"mqtt_conflags\", StringType(), True),\n","    StructField(\"mqtt_dupflag\", IntegerType(), True),\n","    StructField(\"mqtt_hdrflags\", StringType(), True),\n","    StructField(\"mqtt_kalive\", IntegerType(), True),\n","    StructField(\"mqtt_len\", IntegerType(), True),\n","    StructField(\"mqtt_msg\", StringType(), True),\n","    StructField(\"mqtt_msgid\", IntegerType(), True),\n","    StructField(\"mqtt_msgtype\", IntegerType(), True),\n","    StructField(\"mqtt_proto_len\", IntegerType(), True),\n","    StructField(\"mqtt_protoname\", StringType(), True),\n","    StructField(\"mqtt_qos\", IntegerType(), True),\n","    StructField(\"mqtt_retain\", IntegerType(), True),\n","    StructField(\"mqtt_sub_qos\", IntegerType(), True),\n","    StructField(\"mqtt_suback_qos\", IntegerType(), True),\n","    StructField(\"mqtt_ver\", IntegerType(), True),\n","    StructField(\"mqtt_willmsg\", IntegerType(), True),\n","    StructField(\"mqtt_willmsg_len\", IntegerType(), True),\n","    StructField(\"mqtt_willtopic\", IntegerType(), True),\n","    StructField(\"mqtt_willtopic_len\", IntegerType(), True),\n","    StructField(\"target\", StringType(), True)\n","])"]},{"cell_type":"markdown","id":"33aef77c","metadata":{},"source":["### Loading data from Cloud bucket and the Postgres on cloud"]},{"cell_type":"code","execution_count":4,"id":"400e827b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df_train = spark.read.csv(\"gs://dataproc-staging-us-central1-159964990471-2n8oqiw8/train70_reduced.csv\" ,header=True, inferSchema= True)\n","df_test = spark.read.csv(\"gs://dataproc-staging-us-central1-159964990471-2n8oqiw8/test30_reduced.csv\", header=True, inferSchema=True)"]},{"cell_type":"code","execution_count":5,"id":"1cd6927c","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import lit\n","\n","df_train = df_train.withColumn('Train', lit(1))\n","df_test = df_test.withColumn('Train', lit(0))"]},{"cell_type":"code","execution_count":6,"id":"c785e97b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/11/16 19:05:43 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]}],"source":["db_properties={}\n","#update your db username\n","db_properties['username']=\"postgres\"\n","#update your db password\n","db_properties['password']=\"18763kebjeseaya\"\n","#make sure you got the right port number here\n","db_properties['url']= \"jdbc:postgresql://34.136.81.58/postgres\"\n","#make sure you had the Postgres JAR file in the right location\n","db_properties['driver']=\"org.postgresql.Driver\"\n","db_properties['table']= \"mqtt\"\n","\n","\n","df_train.write.format(\"jdbc\")\\\n",".mode(\"overwrite\")\\\n",".option(\"url\", db_properties['url'])\\\n",".option(\"dbtable\", db_properties['table'])\\\n",".option(\"user\", db_properties['username'])\\\n",".option(\"password\", db_properties['password'])\\\n",".option(\"Driver\", db_properties['driver'])\\\n",".save()\n","\n","df_test.write.format(\"jdbc\")\\\n",".mode(\"append\")\\\n",".option(\"url\", db_properties['url'])\\\n",".option(\"dbtable\", db_properties['table'])\\\n",".option(\"user\", db_properties['username'])\\\n",".option(\"password\", db_properties['password'])\\\n",".option(\"Driver\", db_properties['driver'])\\\n",".save()"]},{"cell_type":"code","execution_count":7,"id":"8ad00639","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 6:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["-RECORD 0--------------------------------\n"," tcp.flags                  | 0x00000010 \n"," tcp.time_delta             | 1.0E-6     \n"," tcp.len                    | 0          \n"," mqtt.conack.flags          | 0          \n"," mqtt.conack.flags.reserved | 0.0        \n"," mqtt.conack.flags.sp       | 0.0        \n"," mqtt.conack.val            | 0.0        \n"," mqtt.conflag.cleansess     | 0.0        \n"," mqtt.conflag.passwd        | 0.0        \n"," mqtt.conflag.qos           | 0.0        \n"," mqtt.conflag.reserved      | 0.0        \n"," mqtt.conflag.retain        | 0.0        \n"," mqtt.conflag.uname         | 0.0        \n"," mqtt.conflag.willflag      | 0.0        \n"," mqtt.conflags              | 0          \n"," mqtt.dupflag               | 0.0        \n"," mqtt.hdrflags              | 0          \n"," mqtt.kalive                | 0.0        \n"," mqtt.len                   | 0.0        \n"," mqtt.msg                   | 0          \n"," mqtt.msgid                 | 0.0        \n"," mqtt.msgtype               | 0.0        \n"," mqtt.proto_len             | 0.0        \n"," mqtt.protoname             | 0          \n"," mqtt.qos                   | 0.0        \n"," mqtt.retain                | 0.0        \n"," mqtt.sub.qos               | 0.0        \n"," mqtt.suback.qos            | 0.0        \n"," mqtt.ver                   | 0.0        \n"," mqtt.willmsg               | 0.0        \n"," mqtt.willmsg_len           | 0.0        \n"," mqtt.willtopic             | 0.0        \n"," mqtt.willtopic_len         | 0.0        \n"," target                     | dos        \n"," Train                      | 1          \n","only showing top 1 row\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df = sqlContext.read.format(\"jdbc\")\\\n","    .option(\"url\", db_properties['url'])\\\n","    .option(\"dbtable\", db_properties['table'])\\\n","    .option(\"user\", db_properties['username'])\\\n","    .option(\"password\", db_properties['password'])\\\n","    .option(\"Driver\", db_properties['driver'])\\\n","    .load()\n","\n","df.show(1, vertical=True)"]},{"cell_type":"code","execution_count":8,"id":"394ad9c3","metadata":{},"outputs":[],"source":["df_train = df.filter(df.Train == 1)\n","df_test = df.filter(df.Train == 0)\n","\n","#dropping the train column\n","df_train = df_train.drop('Train')\n","df_test = df_test.drop('Train')"]},{"cell_type":"code","execution_count":9,"id":"3989f0c2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- tcp_flags: string (nullable = true)\n"," |-- tcp_time_delta: double (nullable = true)\n"," |-- tcp_len: integer (nullable = true)\n"," |-- mqtt_conack_flags: string (nullable = true)\n"," |-- mqtt_conack_flags_reserved: double (nullable = true)\n"," |-- mqtt_conack_flags_sp: double (nullable = true)\n"," |-- mqtt_conack_val: double (nullable = true)\n"," |-- mqtt_conflag_cleansess: double (nullable = true)\n"," |-- mqtt_conflag_passwd: double (nullable = true)\n"," |-- mqtt_conflag_qos: double (nullable = true)\n"," |-- mqtt_conflag_reserved: double (nullable = true)\n"," |-- mqtt_conflag_retain: double (nullable = true)\n"," |-- mqtt_conflag_uname: double (nullable = true)\n"," |-- mqtt_conflag_willflag: double (nullable = true)\n"," |-- mqtt_conflags: string (nullable = true)\n"," |-- mqtt_dupflag: double (nullable = true)\n"," |-- mqtt_hdrflags: string (nullable = true)\n"," |-- mqtt_kalive: double (nullable = true)\n"," |-- mqtt_len: double (nullable = true)\n"," |-- mqtt_msg: string (nullable = true)\n"," |-- mqtt_msgid: double (nullable = true)\n"," |-- mqtt_msgtype: double (nullable = true)\n"," |-- mqtt_proto_len: double (nullable = true)\n"," |-- mqtt_protoname: string (nullable = true)\n"," |-- mqtt_qos: double (nullable = true)\n"," |-- mqtt_retain: double (nullable = true)\n"," |-- mqtt_sub_qos: double (nullable = true)\n"," |-- mqtt_suback_qos: double (nullable = true)\n"," |-- mqtt_ver: double (nullable = true)\n"," |-- mqtt_willmsg: double (nullable = true)\n"," |-- mqtt_willmsg_len: double (nullable = true)\n"," |-- mqtt_willtopic: double (nullable = true)\n"," |-- mqtt_willtopic_len: double (nullable = true)\n"," |-- target: string (nullable = true)\n"," |-- Train: integer (nullable = true)\n","\n"]}],"source":["cols = df.columns\n","\n","for column in cols:\n","    new_column = column.replace('.', '_')\n","    df = df.withColumnRenamed(column, new_column)\n","    \n","df.printSchema()"]},{"cell_type":"code","execution_count":10,"id":"1a9bcaef","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["231646\n","99290\n","330936 330936\n"]}],"source":["print(df_train.count())\n","print(df_test.count())\n","print(f'{df.count()} {df_train.count()+df_test.count()}')"]},{"cell_type":"markdown","id":"1c1629eb","metadata":{},"source":["### Task-II: Conduct analytics on your dataset (20% of course project grade)\n","Develop Python functions that run Spark to answer the following questions. All of the core analysis and data ingestion should be conducted via PySpark. Ingest all the data to answer the following questions from the Postgres Database table.\n","1. What is the average length of an MQTT message captured in the training dataset?\n","2. For each target value, what is the average length of the TCP message? (Conduct this process programmatically and don’t hardcode any of the target values in your command)\n","3. Build a Python function that uses PySpark to list the most frequent X TCP flags where X is a user-provided parameter.\n","    o Make sure to handle this scenario as well: if the user requests 5 most frequent TCP flags but there are 3 Flags that share the same count at rank number 5, please include all of them in your output.\n","4. Among the listed targets, what is the most popular target on Google News? (Use 5-minutes Google News feed to justify your answer).\n","    o Use this query: https://news.google.com/rss/search?q=popular+cyber+attacks\n","    o You may find yourself in need to decrypt the target values in the dataset to proper English equivalent. For example, “bruteforce” to “brute force”."]},{"cell_type":"code","execution_count":11,"id":"b5e67b10","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|summary|          mqtt_len|\n","+-------+------------------+\n","|   mean|31.435725201384873|\n","+-------+------------------+\n","\n"]}],"source":["# (1)\n","from pyspark.sql.functions import length, avg\n","from pyspark.sql.functions import col\n","\n","df_train = df.where(df.Train == 1)\n","# print(df_train.count())\n","df_mqtt_msg_avg = df_train.select(col('mqtt_len')).summary(\"mean\")\n","df_mqtt_msg_avg.show()\n","\n"]},{"cell_type":"code","execution_count":12,"id":"4f6c97b9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|summary|          mqtt_len|\n","+-------+------------------+\n","|   mean|31.435725201384873|\n","+-------+------------------+\n","\n"]}],"source":["df_mqtt_msg_avg.show()"]},{"cell_type":"markdown","id":"fc1fd1a2","metadata":{},"source":["### Correcting this from the comment received at checkpoint submission"]},{"cell_type":"code","execution_count":13,"id":"2cd0d134","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-------+\n","|    target|tcp_len|\n","+----------+-------+\n","|       dos|      0|\n","|legitimate|     14|\n","|   slowite|      0|\n","|legitimate|      0|\n","|       dos|      8|\n","+----------+-------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 26:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+------------------+\n","|    target|   tcp_msg_avg_len|\n","+----------+------------------+\n","|   slowite|3.9993479678330797|\n","|bruteforce|3.9871043376318873|\n","|     flood|13313.415986949429|\n","| malformed| 20.97491761259612|\n","|       dos|312.65759830457716|\n","|legitimate| 7.776101001432345|\n","+----------+------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# (2)\n","\n","\n","df.select('target', 'tcp_len').show(5)\n","\n","# Correcting this from the comment received at checkpoint submission\n","\n","df_tcp_msg_avg_len = df.groupBy('target').agg(avg('tcp_len').alias('tcp_msg_avg_len'))\n","df_tcp_msg_avg_len.show()"]},{"cell_type":"code","execution_count":14,"id":"1cf5510f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+--------+\n","|    target|mqtt_len|\n","+----------+--------+\n","|       dos|     0.0|\n","|legitimate|    12.0|\n","|   slowite|     0.0|\n","|legitimate|     0.0|\n","|       dos|     2.0|\n","+----------+--------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 30:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+------------------+\n","|    target|     avg(mqtt_len)|\n","+----------+------------------+\n","|   slowite|3.5331449684851117|\n","|bruteforce|2.9956554720364115|\n","|     flood| 7.491027732463295|\n","| malformed| 6.263181984621018|\n","|       dos| 71.13934256294488|\n","|legitimate| 6.515196750935254|\n","+----------+------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# (2)\n","\n","\n","df.select('target', 'mqtt_len').show(5)\n","\n","df_tcp_msg_avg_len = df.groupBy('target').agg(avg('mqtt_len')).alias('mqtt_msg_avg_len')\n","df_tcp_msg_avg_len.show()"]},{"cell_type":"code","execution_count":15,"id":"b47620c3","metadata":{},"outputs":[],"source":["# num_distinct_counts = df_tcp_flags.select('count').count()\n","# num_distinct_counts"]},{"cell_type":"code","execution_count":16,"id":"a5d3aab7","metadata":{},"outputs":[],"source":["# from pyspark.sql.functions import sum, col, desc\n","# df_tcp_flags = df.groupBy('tcp_flags').count().orderBy(desc('count'))\n","# x = 5\n","# num_to_show = 0\n","# num_distint = 1\n","\n","# values_list = df_tcp_flags.select('count').rdd.map(lambda row: row[0]).collect()\n","# # len(values_list)\n","# while i < len(values_list):\n","#     while values_list[i] == values_list[i+1]:\n","#         i += 1\n","#     num_distint += 1\n","    \n","    \n","    \n","\n","# # df_tcp_flags.show()"]},{"cell_type":"code","execution_count":17,"id":"c5587dc5","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import desc\n","\n","def get_frequent_tcp_flags(x, df):\n","    df_temp = df.groupBy('tcp_flags').count().orderBy(desc('count'))\n","    num_distinct_counts = df_temp.select('count').count()\n","    if x > num_distinct_counts:\n","        print('x out of bounds, give a smaller number')\n","        return\n","    print(df_temp.show(x))\n","#     values_list = df_tcp_flags.select('count').rdd.map(lambda row: row[0]).collect()"]},{"cell_type":"code","execution_count":18,"id":"b5e65727","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+-----+\n","|tcp_flags|count|\n","+---------+-----+\n","+---------+-----+\n","only showing top 0 rows\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","+----------+------+\n","only showing top 1 row\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","|0x00000010|134547|\n","+----------+------+\n","only showing top 2 rows\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","|0x00000010|134547|\n","|0x00000011|  4198|\n","+----------+------+\n","only showing top 3 rows\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","|0x00000010|134547|\n","|0x00000011|  4198|\n","|0x00000002|  3372|\n","+----------+------+\n","only showing top 4 rows\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","|0x00000010|134547|\n","|0x00000011|  4198|\n","|0x00000012|  3372|\n","|0x00000002|  3372|\n","+----------+------+\n","only showing top 5 rows\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","|0x00000010|134547|\n","|0x00000011|  4198|\n","|0x00000012|  3372|\n","|0x00000002|  3372|\n","|0x00000004|  1592|\n","+----------+------+\n","only showing top 6 rows\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","|0x00000010|134547|\n","|0x00000011|  4198|\n","|0x00000012|  3372|\n","|0x00000002|  3372|\n","|0x00000004|  1592|\n","|0x00000019|   738|\n","+----------+------+\n","only showing top 7 rows\n","\n","None\n","+----------+------+\n","| tcp_flags| count|\n","+----------+------+\n","|0x00000018|183076|\n","|0x00000010|134547|\n","|0x00000011|  4198|\n","|0x00000012|  3372|\n","|0x00000002|  3372|\n","|0x00000004|  1592|\n","|0x00000019|   738|\n","|0x00000014|    41|\n","+----------+------+\n","\n","None\n","x out of bounds, give a smaller number\n"]}],"source":["for x in range(10):\n","    get_frequent_tcp_flags(x, df)"]},{"cell_type":"code","execution_count":19,"id":"5ded6550","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: confluent-kafka in /opt/conda/miniconda3/lib/python3.10/site-packages (2.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: feedparser in /opt/conda/miniconda3/lib/python3.10/site-packages (6.0.10)\n","Requirement already satisfied: sgmllib3k in /opt/conda/miniconda3/lib/python3.10/site-packages (from feedparser) (1.0.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: nltk in /opt/conda/miniconda3/lib/python3.10/site-packages (3.6.7)\n","Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (1.1.0)\n","Requirement already satisfied: regex>=2021.8.3 in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (2022.8.17)\n","Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (4.66.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: seaborn in /opt/conda/miniconda3/lib/python3.10/site-packages (0.12.2)\n","Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/miniconda3/lib/python3.10/site-packages (from seaborn) (1.22.4)\n","Requirement already satisfied: pandas>=0.25 in /opt/conda/miniconda3/lib/python3.10/site-packages (from seaborn) (1.4.4)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from seaborn) (3.5.3)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.2.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install confluent-kafka\n","!pip install feedparser\n","!pip install nltk\n","!pip install seaborn"]},{"cell_type":"markdown","id":"468392d9","metadata":{},"source":["### From new Kafka account"]},{"cell_type":"code","execution_count":20,"id":"87185d4a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["%4|1700161588.361|CONFWARN|cluster-9596-m#producer-1| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n","%4|1700161588.361|CONFWARN|cluster-9596-m#producer-1| [thrd:app]: Configuration property session.timeout.ms is a consumer property and will be ignored by this producer instance\n","%4|1700161588.361|CONFWARN|cluster-9596-m#producer-1| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n"]}],"source":["from confluent_kafka import Producer\n","import socket\n","#Initialize Your Parameters here - Keep the variable values as is for the ones you can't find on the Confluent-Kafka connection \n","KAFKA_CONFIG = {\n","    \"bootstrap.servers\":\"pkc-6ojv2.us-west4.gcp.confluent.cloud:9092\",\n","    \"security.protocol\":\"SASL_SSL\",\n","    \"sasl.mechanisms\":\"PLAIN\",\n","    \"sasl.username\":\"W7IXLJ6KRDZDCGIX\",\n","    \"sasl.password\":\"31A2pIPznXDYyKik4ZBMfCIWfr5Jc37LtqCNtRdd3m1faPWZyU7PKKV8vT3EDn8T\",\n","    \"session.timeout.ms\":\"45000\",\n","    \"group.id\":\"python-group-1\",\n","    'auto.offset.reset': 'smallest',\n","    'client.id': socket.gethostname()\n","}\n","\n","# Update your topic name\n","topic_name = \"topic_0\"\n","producer = Producer(KAFKA_CONFIG)\n"]},{"cell_type":"code","execution_count":21,"id":"440dac32","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Start time: 0 sec...\n","End time: 60.00004482269287 sec...\n"]}],"source":["import feedparser\n","import time\n","\n","# We are searching for Analytics in the news\n","feed_url = \"https://news.google.com/rss/search?q=popular+cyber+attacks\"\n","duration_minutes = 1\n","\n","\n","def extract_news_feed(feed_url):\n","    start_time = time.time()\n","    end_time = start_time + (duration_minutes * 60)\n","    \n","    feed = feedparser.parse(feed_url)\n","    articles = []\n","    extracted_articles = set()\n","    \n","    seconds = 60\n","    print(f'Start time: {0} sec...')\n","    while time.time() < end_time:\n","        for entry in feed.entries:\n","            if time.time() >= end_time:\n","                break\n","            if time.time()-start_time > seconds:\n","                print(f'Checkpoint {time.time()-start_time} sec...')\n","                seconds += 60\n","            link = entry.link\n","            title = entry.title.encode('ascii', 'ignore').decode()\n","            unique_id = f'{link}-{title}'\n","            if unique_id in extracted_articles:\n","                continue\n","            extracted_articles.add(unique_id)\n","            article_data = {\"title\": title, \"link\":link}\n","            if article_data is not None:\n","                producer.produce(topic_name, key=article_data[\"title\"], value=article_data[\"link\"])\n","        producer.flush()\n","    print(f'End time: {time.time()-start_time} sec...')\n","\n","    \n","extract_news_feed(feed_url)"]},{"cell_type":"code","execution_count":22,"id":"b595aa87","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-----+\n","|key|value|\n","+---+-----+\n","+---+-----+\n","\n","Waiting...\n","Waiting...\n","Waiting...\n","Waiting...\n","Waiting...\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 123:=====================================================> (28 + 1) / 29]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|                 key|               value|\n","+--------------------+--------------------+\n","|hhs office for ci...|https://news.goog...|\n","|blackcataplhv ran...|https://news.goog...|\n","|8 phishing techni...|https://news.goog...|\n","|uae has thwarted ...|https://news.goog...|\n","|cobalt iron secur...|https://news.goog...|\n","|cybercriminals di...|https://news.goog...|\n","|pcij website goes...|https://news.goog...|\n","|ai could worsen c...|https://news.goog...|\n","|china blamed as m...|https://news.goog...|\n","|13th november thr...|https://news.goog...|\n","|sherweb teams wit...|https://news.goog...|\n","|fbi struggled to ...|https://news.goog...|\n","|openais chatgpt t...|https://news.goog...|\n","|lindy cameron at ...|https://news.goog...|\n","|10 most common ty...|https://news.goog...|\n","|next steps in pre...|https://news.goog...|\n","|uncovering the un...|https://news.goog...|\n","|the 10 biggest cy...|https://news.goog...|\n","|cloroxs cyber chi...|https://news.goog...|\n","|fcc proposes 3yea...|https://news.goog...|\n","+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from confluent_kafka import Consumer\n","from pyspark.sql.types import *\n","import string\n","import time\n","\n","\n","# Clean the punctation by making a translation table that maps punctations to empty strings\n","translator = str.maketrans(\"\", \"\", string.punctuation)\n","\n","\n","emp_RDD = spark.sparkContext.emptyRDD()\n","# Defining the schema of the DataFrame\n","columns = StructType([StructField('key', StringType(), False),\n","                      StructField('value', StringType(), False)])\n","\n","# Creating an empty DataFrame\n","df2 = spark.createDataFrame(data=emp_RDD,\n","                                   schema=columns)\n"," \n","# Printing the DataFrame with no data\n","df2.show()\n","\n","consumer = Consumer(KAFKA_CONFIG)\n","consumer.subscribe([topic_name])\n","\n","try:\n","    i = 0\n","    while i < 5:\n","        msg = consumer.poll(timeout=1.0)\n","        if msg is None:\n","            i = i + 1\n","            print(\"Waiting...\")\n","            continue\n","        if msg is not None:\n","            key = msg.key().decode('utf-8').lower().translate(translator)\n","            cleaned_key = \" \".join(key.split())\n","            value = msg.value().decode('utf-8')\n","            added_row = [[cleaned_key,value]]\n","            added_df2 = spark.createDataFrame(added_row, columns)\n","            df2 = df2.union(added_df2)\n","\n","\n","\n","except KeyboardInterrupt:\n","    pass\n","finally:\n","    consumer.close()\n","    df2.show()\n"]},{"cell_type":"code","execution_count":23,"id":"15d96b35","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 124:===================================================> (192 + 4) / 196]\r"]},{"name":"stdout","output_type":"stream","text":["+-----+-----+\n","| word|count|\n","+-----+-----+\n","|cyber|   68|\n","| ddos|    1|\n","+-----+-----+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["## scan full dataset\n","from pyspark.sql.functions import *\n","# response = table.scan(AttributesToGet=['protocol_type'])\n","# items = response['Items']\n","\n","# while 'LastEvaluatedKey' in response:\n","#     response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n","#     items.extend(response['Items'])\n","    \n","# response['Items'] = items\n","    \n","# unique_values = set()\n","\n","# if 'Items' in response:\n","#     for item in response['Items']:\n","#         if 'protocol_type' in item:\n","#             unique_values.add(item['protocol_type'])\n","\n","attacks = ['slowite', 'brute force', 'flood', 'malformed', 'dos', 'ddos', 'legitimate', 'denial-of-service', 'cyber', 'hack']\n","\n","\n","streamed_data = df2.withColumn('word', explode(split(col('key'), ' '))) \\\n","                .filter(col('word').isin(attacks)) \\\n","                .groupBy('word') \\\n","                .count() \\\n","                .sort('count', ascending=False)\n","    \n","streamed_data.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
