{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7d4c79",
   "metadata": {},
   "source": [
    "### Task-I: Build and populate necessary tables (30% of course project grade)\n",
    "1) Ingest both train and test data into one Postgres Database table. Use the augmented datasets that are provided under Final CSV folder.\n",
    "2) Add a field to your database table that distinguishes between train and test datasets.\n",
    "3) Identify constraints as needed and document them in your Readme.md file.\n",
    "4) Your tables should be created in schema with the name “mqtt”.\n",
    "5) In your ReadMe.md, add a description for the features in the dataset.\n",
    "6) Use the reduced version of the data if your laptop’s memory can’t handle the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d6158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines if you are using Windows!\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "appName = \"Big Data Analytics\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec10af5",
   "metadata": {},
   "source": [
    "This Schema has been created after manually observing the different values in the .csv file. For Hexadecimal vals, string type is used and for mqtt_msg string type is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8663e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "CREATE SCHEMA mqtt;\n",
    "\"\"\")\n",
    "\n",
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99cfd758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "\n",
    "mqtt = StructType([\n",
    "    StructField(\"tcp_flags\", StringType(), True),\n",
    "    StructField(\"tcp_time_delta\", DoubleType(), True),\n",
    "    StructField(\"tcp_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conack_flags\", StringType(), True),\n",
    "    StructField(\"mqtt_conack_flags_reserved\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conack_flags_sp\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conack_val\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_cleansess\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_passwd\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_reserved\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_retain\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_uname\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_willflag\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflags\", StringType(), True),\n",
    "    StructField(\"mqtt_dupflag\", IntegerType(), True),\n",
    "    StructField(\"mqtt_hdrflags\", StringType(), True),\n",
    "    StructField(\"mqtt_kalive\", IntegerType(), True),\n",
    "    StructField(\"mqtt_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_msg\", StringType(), True),\n",
    "    StructField(\"mqtt_msgid\", IntegerType(), True),\n",
    "    StructField(\"mqtt_msgtype\", IntegerType(), True),\n",
    "    StructField(\"mqtt_proto_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_protoname\", StringType(), True),\n",
    "    StructField(\"mqtt_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_retain\", IntegerType(), True),\n",
    "    StructField(\"mqtt_sub_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_suback_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_ver\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willmsg\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willmsg_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willtopic\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willtopic_len\", IntegerType(), True),\n",
    "    StructField(\"target\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400e827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = spark.read.csv(\"train70_reduced.csv\" ,header=True, inferSchema= False)\n",
    "# df_test = spark.read.csv(\"test30_reduced.csv\", header=True, inferSchema=False)\n",
    "\n",
    "# df_train = spark.read.csv(\"train70_reduced.csv\" ,header=True, inferSchema= False).schema(mqtt)\n",
    "# df_test = spark.read.csv(\"test30_reduced.csv\", header=True, inferSchema=False).schema(mqtt)\n",
    "\n",
    "df_train = spark.read.format('csv').schema(mqtt).option('header', True).load('train70_reduced.csv')\n",
    "df_test = spark.read.format('csv').schema(mqtt).option('header', True).load('test30_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd6927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_train = df_train.withColumn('Train', lit(1))\n",
    "df_test = df_test.withColumn('Train', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c785e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "#update your db username\n",
    "db_properties['username']=\"postgres\"\n",
    "#update your db password\n",
    "db_properties['password']=\"18763kebjeseaya\"\n",
    "#make sure you got the right port number here\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "#make sure you had the Postgres JAR file in the right location\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "db_properties['table']= \"mqtt\"\n",
    "\n",
    "\n",
    "df_train.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n",
    "\n",
    "df_test.write.format(\"jdbc\")\\\n",
    ".mode(\"append\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad00639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = sqlContext.read.format(\"jdbc\")\\\n",
    "#     .option(\"url\", db_properties['url'])\\\n",
    "#     .option(\"dbtable\", db_properties['table'])\\\n",
    "#     .option(\"user\", db_properties['username'])\\\n",
    "#     .option(\"password\", db_properties['password'])\\\n",
    "#     .option(\"Driver\", db_properties['driver'])\\\n",
    "#     .load()\n",
    "\n",
    "# df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3989f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = df.columns\n",
    "\n",
    "# for column in cols:\n",
    "#     new_column = column.replace('.', '_')\n",
    "#     df = df.withColumnRenamed(column, new_column)\n",
    "    \n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9bcaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231646\n",
      "99290\n"
     ]
    }
   ],
   "source": [
    "print(df_train.count())\n",
    "print(df_test.count())\n",
    "# print(f'{df.count()} {df_train.count()+df_test.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1629eb",
   "metadata": {},
   "source": [
    "### Task-II: Conduct analytics on your dataset (20% of course project grade)\n",
    "Develop Python functions that run Spark to answer the following questions. All of the core analysis and data ingestion should be conducted via PySpark. Ingest all the data to answer the following questions from the Postgres Database table.\n",
    "1. What is the average length of an MQTT message captured in the training dataset?\n",
    "2. For each target value, what is the average length of the TCP message? (Conduct this process programmatically and don’t hardcode any of the target values in your command)\n",
    "3. Build a Python function that uses PySpark to list the most frequent X TCP flags where X is a user-provided parameter.\n",
    "    o Make sure to handle this scenario as well: if the user requests 5 most frequent TCP flags but there are 3 Flags that share the same count at rank number 5, please include all of them in your output.\n",
    "4. Among the listed targets, what is the most popular target on Google News? (Use 5-minutes Google News feed to justify your answer).\n",
    "    o Use this query: https://news.google.com/rss/search?q=popular+cyber+attacks\n",
    "    o You may find yourself in need to decrypt the target values in the dataset to proper English equivalent. For example, “bruteforce” to “brute force”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16890d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " tcp_flags                  | 0x00000018 \n",
      " tcp_time_delta             | 0.998867   \n",
      " tcp_len                    | 10         \n",
      " mqtt_conack_flags          | 0          \n",
      " mqtt_conack_flags_reserved | null       \n",
      " mqtt_conack_flags_sp       | null       \n",
      " mqtt_conack_val            | null       \n",
      " mqtt_conflag_cleansess     | null       \n",
      " mqtt_conflag_passwd        | null       \n",
      " mqtt_conflag_qos           | null       \n",
      " mqtt_conflag_reserved      | null       \n",
      " mqtt_conflag_retain        | null       \n",
      " mqtt_conflag_uname         | null       \n",
      " mqtt_conflag_willflag      | null       \n",
      " mqtt_conflags              | 0          \n",
      " mqtt_dupflag               | null       \n",
      " mqtt_hdrflags              | 0x00000030 \n",
      " mqtt_kalive                | null       \n",
      " mqtt_len                   | null       \n",
      " mqtt_msg                   | 32         \n",
      " mqtt_msgid                 | null       \n",
      " mqtt_msgtype               | null       \n",
      " mqtt_proto_len             | null       \n",
      " mqtt_protoname             | 0          \n",
      " mqtt_qos                   | null       \n",
      " mqtt_retain                | null       \n",
      " mqtt_sub_qos               | null       \n",
      " mqtt_suback_qos            | null       \n",
      " mqtt_ver                   | null       \n",
      " mqtt_willmsg               | null       \n",
      " mqtt_willmsg_len           | null       \n",
      " mqtt_willtopic             | null       \n",
      " mqtt_willtopic_len         | null       \n",
      " target                     | legitimate \n",
      " Train                      | 1          \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5e67b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of the mqtt msg in training set is 33.87447657201074\n"
     ]
    }
   ],
   "source": [
    "# (1)\n",
    "from pyspark.sql.functions import length, avg\n",
    "\n",
    "def avg_len_mqtt_msg_train(df):\n",
    "    df_train = df[df['Train'] == 1]\n",
    "    df_mqtt_msg_avg = df_train.agg(avg(length('mqtt_msg')).alias('mqtt_avg_len'))\n",
    "    avg_length = df_mqtt_msg_avg.collect()[0][\"mqtt_avg_len\"]\n",
    "    print(f'Average length of the mqtt msg in training set is {avg_length}')\n",
    "    \n",
    "avg_len_mqtt_msg_train(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd34e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|    target|   tcp_msg_avg_len|\n",
      "+----------+------------------+\n",
      "|   slowite|1.1112801564877202|\n",
      "|bruteforce|1.1034411419902075|\n",
      "|     flood|2.8890701468189235|\n",
      "| malformed|1.2652874404979861|\n",
      "|       dos|2.0167238718297207|\n",
      "|legitimate|1.6122577252920594|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (2)\n",
    "\n",
    "def avg_len_tcp_cmd_all_targets(df):\n",
    "#     df.select('target', 'tcp_len').show(5)\n",
    "    df_tcp_msg_avg_len = df.groupBy('target').agg(avg(length('tcp_len')).alias('tcp_msg_avg_len'))\n",
    "    df_tcp_msg_avg_len.show()\n",
    "    \n",
    "avg_len_tcp_cmd_all_targets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5587dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is too small, give a legitimate number\n",
      "1 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "2 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "+----------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "3 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "|0x00000011|  4198|\n",
      "+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "4 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "|0x00000011|  4198|\n",
      "|0x00000002|  3372|\n",
      "+----------+------+\n",
      "only showing top 4 rows\n",
      "\n",
      "None\n",
      "5 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "|0x00000011|  4198|\n",
      "|0x00000002|  3372|\n",
      "|0x00000012|  3372|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "6 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "|0x00000011|  4198|\n",
      "|0x00000002|  3372|\n",
      "|0x00000012|  3372|\n",
      "|0x00000004|  1592|\n",
      "+----------+------+\n",
      "only showing top 6 rows\n",
      "\n",
      "None\n",
      "7 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "|0x00000011|  4198|\n",
      "|0x00000002|  3372|\n",
      "|0x00000012|  3372|\n",
      "|0x00000004|  1592|\n",
      "|0x00000019|   738|\n",
      "+----------+------+\n",
      "only showing top 7 rows\n",
      "\n",
      "None\n",
      "8 most frequent tcp flags are given below\n",
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "|0x00000011|  4198|\n",
      "|0x00000002|  3372|\n",
      "|0x00000012|  3372|\n",
      "|0x00000004|  1592|\n",
      "|0x00000019|   738|\n",
      "|0x00000014|    41|\n",
      "+----------+------+\n",
      "\n",
      "None\n",
      "9 is out of bounds, give a smaller number\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "def get_frequent_tcp_flags(x, df):\n",
    "    df_temp = df.groupBy('tcp_flags').count().orderBy(desc('count'))\n",
    "    num_distinct_counts = df_temp.select('count').count()\n",
    "    if x <= 0:\n",
    "        print(f'{x} is too small, give a legitimate number')\n",
    "        return\n",
    "    if x > num_distinct_counts:\n",
    "        print(f'{x} is out of bounds, give a smaller number')\n",
    "        return\n",
    "    print(f'{x} most frequent tcp flags are given below')\n",
    "    print(df_temp.show(x))\n",
    "\n",
    "for x in range(10):\n",
    "    get_frequent_tcp_flags(x, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cdb9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import socket\n",
    "#Initialize Your Parameters here - Keep the variable values as is for the ones you can't find on the Confluent-Kafka connection \n",
    "KAFKA_CONFIG = {\n",
    "    \"bootstrap.servers\":\"pkc-lzvrd.us-west4.gcp.confluent.cloud:9092\",\n",
    "    \"security.protocol\":\"SASL_SSL\",\n",
    "    \"sasl.mechanisms\":\"PLAIN\",\n",
    "    \"sasl.username\":\"SNZEP2LF5EDA6R7A\",\n",
    "    \"sasl.password\":\"kB1YfUEB5YPurgB/Ma451fdyKlE5KNMa7KHtoJ8miKX2/A2qcAnUZLMrSFFAX7NY\",\n",
    "    \"session.timeout.ms\":\"45000\",\n",
    "    \"group.id\":\"python-group-1\",\n",
    "    'auto.offset.reset': 'smallest',\n",
    "    'client.id': socket.gethostname()\n",
    "}\n",
    "\n",
    "# Update your topic name\n",
    "topic_name = \"topic_0\"\n",
    "producer = Producer(KAFKA_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "440dac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 0 sec...\n",
      "Checkpoint 20.00419044494629 sec...\n",
      "Checkpoint 40.014198541641235 sec...\n",
      "Checkpoint 60.014158964157104 sec...\n",
      "Checkpoint 80.00529956817627 sec...\n",
      "Checkpoint 100.00778102874756 sec...\n",
      "Checkpoint 120.0051155090332 sec...\n",
      "Checkpoint 140.0073642730713 sec...\n",
      "Checkpoint 160.01463890075684 sec...\n",
      "Checkpoint 180.00393629074097 sec...\n",
      "Checkpoint 200.00091552734375 sec...\n",
      "Checkpoint 220.00052618980408 sec...\n",
      "Checkpoint 240.00517797470093 sec...\n",
      "Checkpoint 260.01083540916443 sec...\n",
      "Checkpoint 280.01047539711 sec...\n",
      "End time: 300.0124797821045 sec...\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import time\n",
    "\n",
    "# We are searching for Analytics in the news\n",
    "feed_url = \"https://news.google.com/rss/search?q=popular+cyber+attacks\"\n",
    "duration_minutes = 5\n",
    "\n",
    "\n",
    "def extract_news_feed(feed_url):\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + (duration_minutes * 60)\n",
    "    \n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    extracted_articles = set()\n",
    "    \n",
    "    seconds = 20\n",
    "    print(f'Start time: {0} sec...')\n",
    "    while time.time() < end_time:\n",
    "        for entry in feed.entries:\n",
    "            if time.time() >= end_time:\n",
    "                break\n",
    "            if time.time()-start_time > seconds:\n",
    "                print(f'Checkpoint {time.time()-start_time} sec...')\n",
    "                seconds += 20\n",
    "            link = entry.link\n",
    "            title = entry.title.encode('ascii', 'ignore').decode()\n",
    "            unique_id = f'{link}-{title}'\n",
    "            if unique_id in extracted_articles:\n",
    "                continue\n",
    "            extracted_articles.add(unique_id)\n",
    "            article_data = {\"title\": title, \"link\":link}\n",
    "            if article_data is not None:\n",
    "                producer.produce(topic_name, key=article_data[\"title\"], value=article_data[\"link\"])\n",
    "        producer.flush()\n",
    "    print(f'End time: {time.time()-start_time} sec...')\n",
    "\n",
    "    \n",
    "extract_news_feed(feed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b595aa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|16th af san anton...|https://news.goog...|\n",
      "|the cyberwar betw...|https://news.goog...|\n",
      "|nsa releases a re...|https://news.goog...|\n",
      "|honeywell unveils...|https://news.goog...|\n",
      "|second annual pon...|https://news.goog...|\n",
      "|gillibrand encour...|https://news.goog...|\n",
      "|hackers could exp...|https://news.goog...|\n",
      "|cyberattacks know...|https://news.goog...|\n",
      "|microsoft how cyb...|https://news.goog...|\n",
      "|google amazon fac...|https://news.goog...|\n",
      "|why user buyin is...|https://news.goog...|\n",
      "|us electrical gri...|https://news.goog...|\n",
      "|researchers uncov...|https://news.goog...|\n",
      "|suspected cyberat...|https://news.goog...|\n",
      "|cyber attacks on ...|https://news.goog...|\n",
      "|cybersecurity tre...|https://news.goog...|\n",
      "|submarine cable g...|https://news.goog...|\n",
      "|are you making yo...|https://news.goog...|\n",
      "|90 of organisatio...|https://news.goog...|\n",
      "|what is threat in...|https://news.goog...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from pyspark.sql.types import *\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "# Clean the punctation by making a translation table that maps punctations to empty strings\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    "# Defining the schema of the DataFrame\n",
    "columns = StructType([StructField('key', StringType(), False),\n",
    "                      StructField('value', StringType(), False)])\n",
    "\n",
    "# Creating an empty DataFrame\n",
    "df2 = spark.createDataFrame(data=emp_RDD,\n",
    "                                   schema=columns)\n",
    " \n",
    "# Printing the DataFrame with no data\n",
    "df2.show()\n",
    "\n",
    "consumer = Consumer(KAFKA_CONFIG)\n",
    "consumer.subscribe([topic_name])\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            i = i + 1\n",
    "            print(\"Waiting...\")\n",
    "            continue\n",
    "        if msg is not None:\n",
    "            key = msg.key().decode('utf-8').lower().translate(translator)\n",
    "            cleaned_key = \" \".join(key.split())\n",
    "            value = msg.value().decode('utf-8')\n",
    "            added_row = [[cleaned_key,value]]\n",
    "            added_df2 = spark.createDataFrame(added_row, columns)\n",
    "            df2 = df2.union(added_df2)\n",
    "\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "    df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15d96b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## scan full dataset\n",
    "from pyspark.sql.functions import *\n",
    "# response = table.scan(AttributesToGet=['protocol_type'])\n",
    "# items = response['Items']\n",
    "\n",
    "# while 'LastEvaluatedKey' in response:\n",
    "#     response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "#     items.extend(response['Items'])\n",
    "    \n",
    "# response['Items'] = items\n",
    "    \n",
    "# unique_values = set()\n",
    "\n",
    "# if 'Items' in response:\n",
    "#     for item in response['Items']:\n",
    "#         if 'protocol_type' in item:\n",
    "#             unique_values.add(item['protocol_type'])\n",
    "\n",
    "attacks = ['slowite', 'brute force', 'flood', 'malformed', 'dos', 'ddos']\n",
    "# not including \"legitimate\" as it is not an attack\n",
    "\n",
    "\n",
    "streamed_data = df2.withColumn('word', explode(split(col('key'), ' '))) \\\n",
    "                .filter(col('word').isin(attacks)) \\\n",
    "                .groupBy('word') \\\n",
    "                .count() \\\n",
    "                .sort('count', ascending=False)\n",
    "    \n",
    "streamed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d98fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
