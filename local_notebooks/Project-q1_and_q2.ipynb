{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7d4c79",
   "metadata": {},
   "source": [
    "### Task-I: Build and populate necessary tables (30% of course project grade)\n",
    "1) Ingest both train and test data into one Postgres Database table. Use the augmented datasets that are provided under Final CSV folder.\n",
    "2) Add a field to your database table that distinguishes between train and test datasets.\n",
    "3) Identify constraints as needed and document them in your Readme.md file.\n",
    "4) Your tables should be created in schema with the name “mqtt”.\n",
    "5) In your ReadMe.md, add a description for the features in the dataset.\n",
    "6) Use the reduced version of the data if your laptop’s memory can’t handle the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d6158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines if you are using Windows!\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "appName = \"Big Data Analytics\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec10af5",
   "metadata": {},
   "source": [
    "This Schema has been created after manually observing the different values in the .csv file. For Hexadecimal vals, string type is used and for mqtt_msg string type is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cfd758",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "CREATE SCHEMA mqtt;\n",
    "\"\"\")\n",
    "\n",
    "# sqlWay.show()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "\n",
    "mqtt = StructType([\n",
    "    StructField(\"tcp_flags\", StringType(), True),\n",
    "    StructField(\"tcp_time_delta\", DoubleType(), True),\n",
    "    StructField(\"tcp_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conack_flags\", StringType(), True),\n",
    "    StructField(\"mqtt_conack_flags_reserved\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conack_flags_sp\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conack_val\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_cleansess\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_passwd\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_reserved\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_retain\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_uname\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_willflag\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflags\", StringType(), True),\n",
    "    StructField(\"mqtt_dupflag\", IntegerType(), True),\n",
    "    StructField(\"mqtt_hdrflags\", StringType(), True),\n",
    "    StructField(\"mqtt_kalive\", IntegerType(), True),\n",
    "    StructField(\"mqtt_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_msg\", StringType(), True),\n",
    "    StructField(\"mqtt_msgid\", IntegerType(), True),\n",
    "    StructField(\"mqtt_msgtype\", IntegerType(), True),\n",
    "    StructField(\"mqtt_proto_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_protoname\", StringType(), True),\n",
    "    StructField(\"mqtt_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_retain\", IntegerType(), True),\n",
    "    StructField(\"mqtt_sub_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_suback_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_ver\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willmsg\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willmsg_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willtopic\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willtopic_len\", IntegerType(), True),\n",
    "    StructField(\"target\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400e827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv(\"train70_reduced.csv\" ,header=True, inferSchema= True)\n",
    "df_test = spark.read.csv(\"test30_reduced.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# df_train = spark.read.csv(\"train70_reduced.csv\" ,header=True, inferSchema= False).schema(mqtt)\n",
    "# df_test = spark.read.csv(\"test30_reduced.csv\", header=True, inferSchema=False).schema(mqtt)\n",
    "\n",
    "# df_train = spark.read.format('csv').schema(mqtt).option('header', True).load('train70_reduced.csv')\n",
    "# df_test = spark.read.format('csv').schema(mqtt).option('header', True).load('test30_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd6927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_train = df_train.withColumn('Train', lit(1))\n",
    "df_test = df_test.withColumn('Train', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c785e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "#update your db username\n",
    "db_properties['username']=\"postgres\"\n",
    "#update your db password\n",
    "db_properties['password']=\"18763kebjeseaya\"\n",
    "#make sure you got the right port number here\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "#make sure you had the Postgres JAR file in the right location\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "db_properties['table']= \"mqtt\"\n",
    "\n",
    "\n",
    "df_train.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n",
    "\n",
    "df_test.write.format(\"jdbc\")\\\n",
    ".mode(\"append\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad00639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " tcp.flags                  | 0x00000018 \n",
      " tcp.time_delta             | 0.998867   \n",
      " tcp.len                    | 10         \n",
      " mqtt.conack.flags          | 0          \n",
      " mqtt.conack.flags.reserved | 0.0        \n",
      " mqtt.conack.flags.sp       | 0.0        \n",
      " mqtt.conack.val            | 0.0        \n",
      " mqtt.conflag.cleansess     | 0.0        \n",
      " mqtt.conflag.passwd        | 0.0        \n",
      " mqtt.conflag.qos           | 0.0        \n",
      " mqtt.conflag.reserved      | 0.0        \n",
      " mqtt.conflag.retain        | 0.0        \n",
      " mqtt.conflag.uname         | 0.0        \n",
      " mqtt.conflag.willflag      | 0.0        \n",
      " mqtt.conflags              | 0          \n",
      " mqtt.dupflag               | 0.0        \n",
      " mqtt.hdrflags              | 0x00000030 \n",
      " mqtt.kalive                | 0.0        \n",
      " mqtt.len                   | 8.0        \n",
      " mqtt.msg                   | 32         \n",
      " mqtt.msgid                 | 0.0        \n",
      " mqtt.msgtype               | 3.0        \n",
      " mqtt.proto_len             | 0.0        \n",
      " mqtt.protoname             | 0          \n",
      " mqtt.qos                   | 0.0        \n",
      " mqtt.retain                | 0.0        \n",
      " mqtt.sub.qos               | 0.0        \n",
      " mqtt.suback.qos            | 0.0        \n",
      " mqtt.ver                   | 0.0        \n",
      " mqtt.willmsg               | 0.0        \n",
      " mqtt.willmsg_len           | 0.0        \n",
      " mqtt.willtopic             | 0.0        \n",
      " mqtt.willtopic_len         | 0.0        \n",
      " target                     | legitimate \n",
      " Train                      | 1          \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3989f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_flags: string (nullable = true)\n",
      " |-- tcp_time_delta: double (nullable = true)\n",
      " |-- tcp_len: integer (nullable = true)\n",
      " |-- mqtt_conack_flags: string (nullable = true)\n",
      " |-- mqtt_conack_flags_reserved: double (nullable = true)\n",
      " |-- mqtt_conack_flags_sp: double (nullable = true)\n",
      " |-- mqtt_conack_val: double (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: double (nullable = true)\n",
      " |-- mqtt_conflag_passwd: double (nullable = true)\n",
      " |-- mqtt_conflag_qos: double (nullable = true)\n",
      " |-- mqtt_conflag_reserved: double (nullable = true)\n",
      " |-- mqtt_conflag_retain: double (nullable = true)\n",
      " |-- mqtt_conflag_uname: double (nullable = true)\n",
      " |-- mqtt_conflag_willflag: double (nullable = true)\n",
      " |-- mqtt_conflags: string (nullable = true)\n",
      " |-- mqtt_dupflag: double (nullable = true)\n",
      " |-- mqtt_hdrflags: string (nullable = true)\n",
      " |-- mqtt_kalive: double (nullable = true)\n",
      " |-- mqtt_len: double (nullable = true)\n",
      " |-- mqtt_msg: string (nullable = true)\n",
      " |-- mqtt_msgid: double (nullable = true)\n",
      " |-- mqtt_msgtype: double (nullable = true)\n",
      " |-- mqtt_proto_len: double (nullable = true)\n",
      " |-- mqtt_protoname: string (nullable = true)\n",
      " |-- mqtt_qos: double (nullable = true)\n",
      " |-- mqtt_retain: double (nullable = true)\n",
      " |-- mqtt_sub_qos: double (nullable = true)\n",
      " |-- mqtt_suback_qos: double (nullable = true)\n",
      " |-- mqtt_ver: double (nullable = true)\n",
      " |-- mqtt_willmsg: double (nullable = true)\n",
      " |-- mqtt_willmsg_len: double (nullable = true)\n",
      " |-- mqtt_willtopic: double (nullable = true)\n",
      " |-- mqtt_willtopic_len: double (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- Train: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns\n",
    "\n",
    "for column in cols:\n",
    "    new_column = column.replace('.', '_')\n",
    "    df = df.withColumnRenamed(column, new_column)\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9bcaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231646\n",
      "99290\n",
      "330936 330936\n"
     ]
    }
   ],
   "source": [
    "print(df_train.count())\n",
    "print(df_test.count())\n",
    "print(f'{df.count()} {df_train.count()+df_test.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97b7648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x00000019\n",
      "60.000878\n",
      "32768\n",
      "0x00000000\n",
      "0.0\n",
      "0.0\n",
      "5.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0x000000c2\n",
      "1.0\n",
      "0x000000e0\n",
      "65535.0\n",
      "692.0\n",
      "746573747465737474657374\n",
      "10000.0\n",
      "14.0\n",
      "4.0\n",
      "MQTT\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "4.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "slowite\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"{df.agg({column: 'max'}).collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1629eb",
   "metadata": {},
   "source": [
    "### Task-II: Conduct analytics on your dataset (20% of course project grade)\n",
    "Develop Python functions that run Spark to answer the following questions. All of the core analysis and data ingestion should be conducted via PySpark. Ingest all the data to answer the following questions from the Postgres Database table.\n",
    "1. What is the average length of an MQTT message captured in the training dataset?\n",
    "2. For each target value, what is the average length of the TCP message? (Conduct this process programmatically and don’t hardcode any of the target values in your command)\n",
    "3. Build a Python function that uses PySpark to list the most frequent X TCP flags where X is a user-provided parameter.\n",
    "    o Make sure to handle this scenario as well: if the user requests 5 most frequent TCP flags but there are 3 Flags that share the same count at rank number 5, please include all of them in your output.\n",
    "4. Among the listed targets, what is the most popular target on Google News? (Use 5-minutes Google News feed to justify your answer).\n",
    "    o Use this query: https://news.google.com/rss/search?q=popular+cyber+attacks\n",
    "    o You may find yourself in need to decrypt the target values in the dataset to proper English equivalent. For example, “bruteforce” to “brute force”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e67b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          mqtt_len|\n",
      "+-------+------------------+\n",
      "|   mean|31.435725201384873|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (1)\n",
    "from pyspark.sql.functions import length, avg\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_train = df.where(df.Train == 1)\n",
    "# print(df_train.count())\n",
    "df_mqtt_msg_avg = df_train.select(col('mqtt_len')).summary(\"mean\")\n",
    "df_mqtt_msg_avg.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f6c97b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          mqtt_len|\n",
      "+-------+------------------+\n",
      "|   mean|31.435725201384873|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mqtt_msg_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting this from the comment received at checkpoint submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd0d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|    target|tcp_len|\n",
      "+----------+-------+\n",
      "|legitimate|     10|\n",
      "|       dos|   1460|\n",
      "|       dos|   1460|\n",
      "|legitimate|     10|\n",
      "|       dos|     16|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------------------+\n",
      "|    target|   tcp_msg_avg_len|\n",
      "+----------+------------------+\n",
      "|   slowite|3.9993479678330797|\n",
      "|bruteforce|3.9871043376318873|\n",
      "|     flood|13313.415986949429|\n",
      "| malformed| 20.97491761259612|\n",
      "|       dos|312.65759830457716|\n",
      "|legitimate| 7.776101001432345|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (2)\n",
    "\n",
    "\n",
    "df.select('target', 'tcp_len').show(5)\n",
    "\n",
    "# Correcting this from the comment received at checkpoint submission\n",
    "\n",
    "df_tcp_msg_avg_len = df.groupBy('target').agg(avg('tcp_len').alias('tcp_msg_avg_len'))\n",
    "df_tcp_msg_avg_len.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    target|mqtt_len|\n",
      "+----------+--------+\n",
      "|legitimate|     8.0|\n",
      "|       dos|   169.0|\n",
      "|       dos|   163.0|\n",
      "|legitimate|     8.0|\n",
      "|       dos|     2.0|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------------------+\n",
      "|    target|     avg(mqtt_len)|\n",
      "+----------+------------------+\n",
      "|   slowite|3.5331449684851117|\n",
      "|bruteforce|2.9956554720364115|\n",
      "|     flood| 7.491027732463295|\n",
      "| malformed| 6.263181984621018|\n",
      "|       dos| 71.13934256294488|\n",
      "|legitimate| 6.515196750935254|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (2)\n",
    "\n",
    "\n",
    "df.select('target', 'mqtt_len').show(5)\n",
    "\n",
    "df_tcp_msg_avg_len = df.groupBy('target').agg(avg('mqtt_len')).alias('mqtt_msg_avg_len')\n",
    "df_tcp_msg_avg_len.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47620c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_distinct_counts = df_tcp_flags.select('count').count()\n",
    "# num_distinct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d3aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import sum, col, desc\n",
    "# df_tcp_flags = df.groupBy('tcp_flags').count().orderBy(desc('count'))\n",
    "# x = 5\n",
    "# num_to_show = 0\n",
    "# num_distint = 1\n",
    "\n",
    "# values_list = df_tcp_flags.select('count').rdd.map(lambda row: row[0]).collect()\n",
    "# # len(values_list)\n",
    "# while i < len(values_list):\n",
    "#     while values_list[i] == values_list[i+1]:\n",
    "#         i += 1\n",
    "#     num_distint += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# # df_tcp_flags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5587dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "def get_frequent_tcp_flags(x, df):\n",
    "    df_temp = df.groupBy('tcp_flags').count().orderBy(desc('count'))\n",
    "    num_distinct_counts = df_temp.select('count').count()\n",
    "    if x > num_distinct_counts:\n",
    "        print('x out of bounds, give a smaller number')\n",
    "        return\n",
    "    print(df_temp.show(x))\n",
    "#     values_list = df_tcp_flags.select('count').rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e65727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "| tcp_flags| count|\n",
      "+----------+------+\n",
      "|0x00000018|183076|\n",
      "|0x00000010|134547|\n",
      "|0x00000011|  4198|\n",
      "|0x00000002|  3372|\n",
      "|0x00000012|  3372|\n",
      "|0x00000004|  1592|\n",
      "|0x00000019|   738|\n",
      "|0x00000014|    41|\n",
      "+----------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "get_frequent_tcp_flags(8, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cdb9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from confluent_kafka import Producer\n",
    "# import socket\n",
    "# #Initialize Your Parameters here - Keep the variable values as is for the ones you can't find on the Confluent-Kafka connection \n",
    "# KAFKA_CONFIG = {\n",
    "#     \"bootstrap.servers\":\"pkc-lzvrd.us-west4.gcp.confluent.cloud:9092\",\n",
    "#     \"security.protocol\":\"SASL_SSL\",\n",
    "#     \"sasl.mechanisms\":\"PLAIN\",\n",
    "#     \"sasl.username\":\"SNZEP2LF5EDA6R7A\",\n",
    "#     \"sasl.password\":\"kB1YfUEB5YPurgB/Ma451fdyKlE5KNMa7KHtoJ8miKX2/A2qcAnUZLMrSFFAX7NY\",\n",
    "#     \"session.timeout.ms\":\"45000\",\n",
    "#     \"group.id\":\"python-group-1\",\n",
    "#     'auto.offset.reset': 'smallest',\n",
    "#     'client.id': socket.gethostname()\n",
    "# }\n",
    "\n",
    "# # Update your topic name\n",
    "# topic_name = \"topic_0\"\n",
    "# producer = Producer(KAFKA_CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From new Kafka account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import socket\n",
    "#Initialize Your Parameters here - Keep the variable values as is for the ones you can't find on the Confluent-Kafka connection \n",
    "KAFKA_CONFIG = {\n",
    "    \"bootstrap.servers\":\"pkc-6ojv2.us-west4.gcp.confluent.cloud:9092\",\n",
    "    \"security.protocol\":\"SASL_SSL\",\n",
    "    \"sasl.mechanisms\":\"PLAIN\",\n",
    "    \"sasl.username\":\"W7IXLJ6KRDZDCGIX\",\n",
    "    \"sasl.password\":\"31A2pIPznXDYyKik4ZBMfCIWfr5Jc37LtqCNtRdd3m1faPWZyU7PKKV8vT3EDn8T\",\n",
    "    \"session.timeout.ms\":\"45000\",\n",
    "    \"group.id\":\"python-group-1\",\n",
    "    'auto.offset.reset': 'smallest',\n",
    "    'client.id': socket.gethostname()\n",
    "}\n",
    "\n",
    "# Update your topic name\n",
    "topic_name = \"topic_0\"\n",
    "producer = Producer(KAFKA_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "440dac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 0 sec...\n",
      "Checkpoint 60.000038862228394 sec...\n",
      "Checkpoint 120.00352311134338 sec...\n",
      "Checkpoint 180.00857782363892 sec...\n",
      "Checkpoint 240.0008602142334 sec...\n",
      "End time: 300.0000092983246 sec...\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import time\n",
    "\n",
    "# We are searching for Analytics in the news\n",
    "feed_url = \"https://news.google.com/rss/search?q=popular+cyber+attacks\"\n",
    "duration_minutes = 5\n",
    "\n",
    "\n",
    "def extract_news_feed(feed_url):\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + (duration_minutes * 60)\n",
    "    \n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    extracted_articles = set()\n",
    "    \n",
    "    seconds = 60\n",
    "    print(f'Start time: {0} sec...')\n",
    "    while time.time() < end_time:\n",
    "        for entry in feed.entries:\n",
    "            if time.time() >= end_time:\n",
    "                break\n",
    "            if time.time()-start_time > seconds:\n",
    "                print(f'Checkpoint {time.time()-start_time} sec...')\n",
    "                seconds += 60\n",
    "            link = entry.link\n",
    "            title = entry.title.encode('ascii', 'ignore').decode()\n",
    "            unique_id = f'{link}-{title}'\n",
    "            if unique_id in extracted_articles:\n",
    "                continue\n",
    "            extracted_articles.add(unique_id)\n",
    "            article_data = {\"title\": title, \"link\":link}\n",
    "            if article_data is not None:\n",
    "                producer.produce(topic_name, key=article_data[\"title\"], value=article_data[\"link\"])\n",
    "        producer.flush()\n",
    "    print(f'End time: {time.time()-start_time} sec...')\n",
    "\n",
    "    \n",
    "extract_news_feed(feed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b595aa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|is the fear of cy...|https://news.goog...|\n",
      "|lockbit hackers p...|https://news.goog...|\n",
      "|the race to adapt...|https://news.goog...|\n",
      "|danish critical i...|https://news.goog...|\n",
      "|cyber security to...|https://news.goog...|\n",
      "|hackers target gr...|https://news.goog...|\n",
      "|spooky cyber stat...|https://news.goog...|\n",
      "|cyber attacks aga...|https://news.goog...|\n",
      "|how to protect ag...|https://news.goog...|\n",
      "|boosting cybersec...|https://news.goog...|\n",
      "|the ico and the n...|https://news.goog...|\n",
      "|customs collabora...|https://news.goog...|\n",
      "|pentesting vs pen...|https://news.goog...|\n",
      "|the 6 most common...|https://news.goog...|\n",
      "|the top 6 cyber s...|https://news.goog...|\n",
      "|the alarming cybe...|https://news.goog...|\n",
      "|director wrays op...|https://news.goog...|\n",
      "|china ukraine and...|https://news.goog...|\n",
      "|the 5 cyber threa...|https://news.goog...|\n",
      "|where cybersecuri...|https://news.goog...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from pyspark.sql.types import *\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "# Clean the punctation by making a translation table that maps punctations to empty strings\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    "# Defining the schema of the DataFrame\n",
    "columns = StructType([StructField('key', StringType(), False),\n",
    "                      StructField('value', StringType(), False)])\n",
    "\n",
    "# Creating an empty DataFrame\n",
    "df2 = spark.createDataFrame(data=emp_RDD,\n",
    "                                   schema=columns)\n",
    " \n",
    "# Printing the DataFrame with no data\n",
    "df2.show()\n",
    "\n",
    "consumer = Consumer(KAFKA_CONFIG)\n",
    "consumer.subscribe([topic_name])\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "    while i < 5:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            i = i + 1\n",
    "            print(\"Waiting...\")\n",
    "            continue\n",
    "        if msg is not None:\n",
    "            key = msg.key().decode('utf-8').lower().translate(translator)\n",
    "            cleaned_key = \" \".join(key.split())\n",
    "            value = msg.value().decode('utf-8')\n",
    "            added_row = [[cleaned_key,value]]\n",
    "            added_df2 = spark.createDataFrame(added_row, columns)\n",
    "            df2 = df2.union(added_df2)\n",
    "\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "    df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15d96b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|cyber|   66|\n",
      "| hack|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## scan full dataset\n",
    "from pyspark.sql.functions import *\n",
    "# response = table.scan(AttributesToGet=['protocol_type'])\n",
    "# items = response['Items']\n",
    "\n",
    "# while 'LastEvaluatedKey' in response:\n",
    "#     response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "#     items.extend(response['Items'])\n",
    "    \n",
    "# response['Items'] = items\n",
    "    \n",
    "# unique_values = set()\n",
    "\n",
    "# if 'Items' in response:\n",
    "#     for item in response['Items']:\n",
    "#         if 'protocol_type' in item:\n",
    "#             unique_values.add(item['protocol_type'])\n",
    "\n",
    "attacks = ['slowite', 'brute force', 'flood', 'malformed', 'dos', 'ddos', 'legitimate', 'denial-of-service', 'cyber', 'hack']\n",
    "\n",
    "\n",
    "streamed_data = df2.withColumn('word', explode(split(col('key'), ' '))) \\\n",
    "                .filter(col('word').isin(attacks)) \\\n",
    "                .groupBy('word') \\\n",
    "                .count() \\\n",
    "                .sort('count', ascending=False)\n",
    "    \n",
    "streamed_data.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
